import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import backend as K

def euclidean_distance(vectors):
    x, y = vectors
    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))

def eucl_dist_output_shape(shapes):
    shape1, shape2 = shapes
    return (shape1[0], 1)

input_shape = (64, 64, 1)  # adjust this based on your input data

# Define the shared network
shared_network = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(filters=64, kernel_size=(10,10), activation='relu', input_shape=input_shape),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Conv2D(filters=128, kernel_size=(7,7), activation='relu'),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Conv2D(filters=128, kernel_size=(4,4), activation='relu'),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Conv2D(filters=256, kernel_size=(4,4), activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(4096, activation='sigmoid'),
    tf.keras.layers.Dropout(0.1)  # Add dropout layer here
])

# Define the two inputs (for the two branches of the Siamese network)
input_a = Input(shape=input_shape)
input_b = Input(shape=input_shape)

# The two inputs pass through the same network
processed_a = shared_network(input_a)
processed_b = shared_network(input_b)

# Compute the distance between the two vectors
distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])

# Define the model
model = Model([input_a, input_b], distance)

# Compile the model
model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])


-------------------------------------------------------------------------------------


import tensorflow as tf
from tensorflow.keras import layers, Model

def build_base_model(input_shape):
    """Function to build base model"""
    input = tf.keras.Input(shape=input_shape)
    x = layers.Dense(64, activation='relu')(input)
    x = layers.Dropout(0.1)(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.1)(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.1)(x)
    x = layers.Dense(512, activation='relu')(x)
    return Model(inputs=input, outputs=x)

def euclidean_distance(vects):
    """Compute Euclidean Distance"""
    x, y = vects
    sum_square = tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True)
    return tf.sqrt(tf.maximum(sum_square, tf.keras.backend.epsilon()))

def contrastive_loss(y_true, y_pred):
    """Contrastive loss function"""
    margin = 1
    square_pred = tf.square(y_pred)
    margin_square = tf.square(tf.maximum(margin - y_pred, 0))
    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)

input_shape = (32,) # This should match your input shape
base_network = build_base_model(input_shape)

input_a = tf.keras.Input(shape=input_shape)
input_b = tf.keras.Input(shape=input_shape)

# because we re-use the same instance 'base_network',
# the weights of the network will be shared across the two branches
processed_a = base_network(input_a)
processed_b = base_network(input_b)

distance = layers.Lambda(euclidean_distance)([processed_a, processed_b])

model = Model([input_a, input_b], distance)

# train
rms = tf.keras.optimizers.RMSprop()
model.compile(loss=contrastive_loss, optimizer=rms)













======================================
tf.random.set_seed(12)
tf.keras.backend.clear_session()
left = np.array(left).astype("float64")
right = np.array(right).astype("float64")
y = np.array(y).astype("float64")

from sklearn.model_selection import train_test_split
left_x_train, left_x_test, left_y_train, left_y_test = train_test_split(left, y, test_size=0.3)
right_x_train, right_x_test, right_y_train, right_y_test = train_test_split(right, y, test_size=0.3)


def build_base_model(input_shape):
    """Function to build base model"""
    input = tf.keras.Input(shape=input_shape)
    x = tf.keras.layers.Dense(128, activation='tanh')(input)
    x = tf.keras.layers.Dropout(0.1)(x)
    x = tf.keras.layers.Dense(64, activation='tanh')(x)
    x = tf.keras.layers.Dropout(0.1)(x)
    x = tf.keras.layers.Dense(32, activation='tanh')(x)
    return Model(inputs=input, outputs=x)

input_shape = (16,) # This should match your input shape
base_network = build_base_model(input_shape)

input_a = tf.keras.Input(shape=input_shape)
input_b = tf.keras.Input(shape=input_shape)

# because we re-use the same instance 'base_network',
# the weights of the network will be shared across the two branches
processed_a = base_network(input_a)
processed_b = base_network(input_b)

# concatenate the processed vectors
concatenated = tf.keras.layers.concatenate([processed_a, processed_b])

# Add a dense layer with a sigmoid unit to generate the similarity score
out = tf.keras.layers.Dense(1, activation="sigmoid")(concatenated)

model = Model([input_a, input_b], out)

# compile the model
model.compile(loss="binary_crossentropy", optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), metrics=["accuracy"])


